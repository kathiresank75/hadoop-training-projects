assignment_kathir.sources=sc1
assignment_kathir.channels=chl 
assignment_kathir.sinks=snk

#configure your components
assignment_kathir.sources.sc1.type=spooldir
assignment_kathir.sources.sc1.channels=chl
assignment_kathir.sources.sc1.spoolDir=/mnt/home/kathiresank/hadoop-training-projects/flume/large_dir_spool/watch_dir
assignment_kathir.sources.sc1.deletePolicy=immediate
assignment_kathir.sources.sc1.includePattern=^prices\.csv$
assignment_kathir.sources.sc1.ignorePattern=(^reports*\.csv$)|(^\.txt$)
assignment_kathir.sources.sc1.fileSuffix=done


# Describing/Configuring the channel
assignment_kathir.channels.chl.type = file
assignment_kathir.channels.chl.capacity = 100000000
assignment_kathir.channels.chl.transactionCapacity = 700
assignment_kathir.channels.chl.checkpointDir=/mnt/home/kathiresank/hadoop-training-projects/flume/large_dir_spool/chkpoint_dir
assignment_kathir.channels.chl.dataDirs=/mnt/home/kathiresank/hadoop-training-projects/flume/large_dir_spool/datadir
assignment_kathir.channels.chl.checkpointInterval=30000	
assignment_kathir.channels.chl.maxFileSize=2146435071	
assignment_kathir.channels.chl.minimumRequiredSpace=524288000	



assignment_kathir.sinks.snk.type=hdfs
assignment_kathir.sinks.snk.channel=chl
assignment_kathir.sinks.snk.hdfs.path=/user/kathiresank/output/handson_train/flume/pystock_data
assignment_kathir.sinks.snk.hdfs.fileType=DataStream
assignment_kathir.sinks.snk.hdfs.writeFormat=Text
assignment_kathir.sinks.snk.hdfs.rollInterval=600
assignment_kathir.sinks.snk.hdfs.rollCount=0
assignment_kathir.sinks.snk.hdfs.rollSize=67108864
assignment_kathir.sinks.snk.hdfs.filePrefix=kathir
assignment_kathir.sinks.snk.hdfs.fileSuffix=.csv